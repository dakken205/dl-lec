{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/dakken205/dl-lec/blob/main/03/03_neuralnetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワーク"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パーセプトロンからニューラルネットワークへ\n",
    "<div style='display: flex;'>\n",
    "    <div  style='width: 400px; margin-right: 10px;'>\n",
    "        <div>\n",
    "            ↓ニューラルネットワークの例\n",
    "        </div>\n",
    "        <div>\n",
    "            <img src='https://image.itmedia.co.jp/ait/articles/2003/24/di-gsdl0102.gif' />\n",
    "        </div>\n",
    "    </div>\n",
    "    <div style='width: 383px;'>\n",
    "        <div>\n",
    "            ↓パーセプトロンの図\n",
    "        </div>\n",
    "        <div>\n",
    "            <img src='https://city3939.com/wp-content/uploads/2018/10/%e3%83%91%e3%83%bc%e3%82%bb%e3%83%97%e3%83%88%e3%83%ad%e3%83%b30.png' />\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "<p>構造はたいして差がないように感じるが。。。</p>\n",
    "\n",
    "### 異なるポイント\n",
    "\n",
    "<div>\n",
    "    <p>１．ニューラルネットワークには、バイアス項というものが存在するようになる</p>\n",
    "    <img style=\"width: 400px;\" src=\"https://miro.medium.com/v2/resize:fit:1100/1*crac_8m0q73ajZ4Nqu3Rzw.jpeg\" />\n",
    "</div>\n",
    "<div>\n",
    "    <p>２．前からの入力を関数（活性化関数）に通してから出力（次の層に伝達）している</p>\n",
    "    <img style=\"width: 400px;\" src=\"https://miraiecosharing1.com/wp-content/uploads/2019/12/image-22.png\" />\n",
    "    <img style=\"width: 400px;\" src=\"https://data-analysis-stats.jp/wp-content/uploads/2021/01/activation-function-01a.png\" />\n",
    "</div>\n",
    "\n",
    "また、パーセプトロンもニューラルネットワークで実装出来ます   \n",
    "$ h(x) = \n",
    "    \\begin{cases}\n",
    "        {0 ~ (x \\le 0)}\\\\\n",
    "        {1 ~ (x > 0)}\n",
    "    \\end{cases}\n",
    "$   \n",
    "$y = h(b + w_1x_1 + w_2x_2)$   \n",
    "\n",
    "<p>なお、パーセプトロンという言葉を利用する際に、正確にこれだということは決まっていないようです</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 活性化関数\n",
    "活性化関数にはいろいろなものがありますが、ここでは以下の３つについてみて行きます\n",
    "- ステップ関数\n",
    "- シグモイド関数\n",
    "- ReLU関数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ステップ関数\n",
    "ステップ関数は、パーセプトロンと同じ機能を果たすための関数であり、すでに、説明したとおりの関数になっています   \n",
    "$ h(x) = \n",
    "    \\begin{cases}\n",
    "        {0 ~ (x \\le 0)}\\\\\n",
    "        {1 ~ (x > 0)}\n",
    "    \\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function_demo(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実は上のコードでは、numpy配列を入力に出来ないので、対応させた関数を実装しなおす\n",
    "def step_function(x: np.ndarray):\n",
    "    y = x > 0\n",
    "    y = y.astype('int')\n",
    "    return y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### シグモイド関数   \n",
    "$$h(x) = \\frac{1}{1 + exp(-x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5.9, 5.9, 100)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(x, sigmoid(x), label='シグモイド関数')\n",
    "plt.plot(x, step_function(x),linestyle='--', label='ステップ関数')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**共通点と異なる点**\n",
    "共通点\n",
    "- おおよそ0~1の範囲に従っている\n",
    "- 非線形\n",
    "異なる点\n",
    "- 滑らかさ\n",
    "\n",
    "ニューラルネットワークでは、活性化関数に、非線形関数を利用しなければなりません。   \n",
    "線形関数（恒等関数など）を用いると、ニューラルネットワークで層を深くすることの意味名が無くなってしまいます"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU関数\n",
    "$ h(x) = \n",
    "    \\begin{cases}\n",
    "        {0 ~ (x \\le 0)}\\\\\n",
    "        {x ~ (x > 0)}\n",
    "    \\end{cases}\n",
    "$   \n",
    "上にあるように，0よりも大きい時はそのままの値を返し，0よりも小さな値ときは，0を返すようにしている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, ReLU(x))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多次元配列の計算\n",
    "### 多次元配列とは\n",
    "数字の集合の事．\n",
    "百聞は一見に如かずということで，コードで色々と実装してみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([1, 2, 3, 4])\n",
    "print('配列の形：\\n',A)\n",
    "print('次元数：', np.ndim(A))\n",
    "print('配列の形：', A.shape)\n",
    "print(A.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([[1,2], [3,4], [5,6]])\n",
    "print('配列の形：\\n', B)\n",
    "print('次元数：', np.ndim(B))\n",
    "print('配列の形：', B.shape)\n",
    "print(B.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多次元配列の計算\n",
    "<img style=\"width: 700px;\" src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/t/takunology/20200303/20200303134613.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[4, 3], [2, 1]])\n",
    "\n",
    "# 行列の積を実装\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "B = np.array([[6, 5], [4, 3], [2, 1]])\n",
    "np.dot(A, B)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 行列計算がそもそも可能かどうかについて\n",
    "**`行列の形状が大切`**   \n",
    "(行数，列数)として計算が可能な形状となるのは   \n",
    "$AB = C$のとき，Aの列数とBの行数が一致しなければ計算できない   \n",
    "出力の形状は，Aの行数とBの列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_1 = np.array([[1], [1], [1]])\n",
    "I_2 = np.array([1, 1, 1])\n",
    "print(A.shape, I_1.shape)\n",
    "print(np.dot(A, I_1), end='\\n\\n')\n",
    "print(A.shape, I_2.shape)\n",
    "print(np.dot(A, I_2), end='\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の例でも分かるように，1次元配列（ベクトル）と2次元配列は計算が可能である．その際，都合の良いような2次元配列に変換され，計算された後，出力も1次元配列に変換されている\n",
    "- 例えば，上の二つ目の例だと，I_2の形状が1x3として認識されると計算できない"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### なぜ行列の計算を紹介するのか\n",
    "それは行列計算によって，ニューラルネットワークの各層の計算が表現されるからである．   \n",
    "$X = \\left[\\begin{matrix} x_1 & x_2 \\end{matrix}\\right]$, `ベクトルであることに注意`   \n",
    "$W = \\left[\\begin{matrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{matrix}\\right]$\n",
    "```python\n",
    "X.shape: (2,)   \n",
    "W.shape: (2, 3)   \n",
    "```\n",
    "このとき，$XWを計算すると$   \n",
    "$XW$   \n",
    "$ =  \\left[\\begin{matrix} x_1 & x_2 \\end{matrix}\\right]\\left[\\begin{matrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{matrix}\\right]$   \n",
    "$ = \\left[\\begin{matrix} x_1w_{11} + x_2w_{12} & x_1w_{21} + w_2w_{22} & x_1w_{31} + x_2w_{32} \\end{matrix}\\right]$   \n",
    "$ = \\left[\\begin{matrix} y_1 & y_2 & y_3\\end{matrix}\\right]$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2])\n",
    "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "Y = np.dot(X, W)\n",
    "print(f'{X.shape} {W.shape} -> {Y.shape}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3層ニューラルネットワークの実装\n",
    "### 記号の確認\n",
    "|第1層の重み付き和|第2層の重み付き和|\n",
    "|-|-|\n",
    "|$A^{(1)}=XW^{(1)}+B^{(1)}$|$A^{(2)}=Z^{(1)}W^{(2)}+B^{(2)}$|\n",
    "\n",
    "$X=\\left[\\begin{matrix} x_1 & x_2 \\end{matrix}\\right]$<br><br>\n",
    "$B^{(1)} = \\left[\\begin{matrix} b_1^{(1)} & b_2^{(1)} & b_3^{(1)}\\end{matrix}\\right]$<br><br>\n",
    "$W^{(1)} = \\left[\\begin{matrix}\n",
    "    w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\\\\n",
    "    w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}\n",
    "\\end{matrix}\\right]$\n",
    "$A^{(1)} = \\left[\\begin{matrix} a_1^{(1)} & a_2^{(1)} & a_3^{(1)}\\end{matrix}\\right]$<br><br>\n",
    "$Z1 = sigmoid(A1)$\n",
    "\n",
    "<p>上の添え字：何番目の層</p>\n",
    "<p>下の添え字の左：次の層の何番目のニューロンか（行先）</p>\n",
    "<p>下の添え字の右：前の層の何番目のニューロンか（どこからか）</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1.0, 0.5])\n",
    "W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "B1 = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "print(X.shape)\n",
    "print(W1.shape)\n",
    "print(B1.shape)\n",
    "\n",
    "A1 = np.dot(X, W1) + B1\n",
    "Z1 = sigmoid(A1)\n",
    "\n",
    "print(A1, end='\\n\\n')\n",
    "\n",
    "# このようにすると，一気に配列内の要素に対してシグモイド関数が適用される\n",
    "print(Z1)\n",
    "print(Z1.shape)\n",
    "print(type(Z1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "B2 = np.array([0.1, 0.2])\n",
    "\n",
    "print(Z1.shape)\n",
    "print(W2.shape)\n",
    "print(B2.shape)\n",
    "\n",
    "A2 = np.dot(Z1, W2) + B2\n",
    "Z2 = sigmoid(A2)\n",
    "\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恒等関数\n",
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "W3 = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "B3 = np.array([0.1, 0.2])\n",
    "\n",
    "A3 = np.dot(Z2, W3)\n",
    "Y = identity_function(A3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワークの出力までをまとめて実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b2'] = np.array([0.1, 0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['b3'] = np.array([0.1, 0.2])\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "def forward(network, X):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    a1 = np.dot(X, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = identity_function(a3)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "network = init_network()\n",
    "x = np.array([1.0, 0.5])\n",
    "y = forward(network, x)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでニューラルネットワークの内部の実装が出来ました．次は，出力の為の層を設計していきます"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力層の設計\n",
    "一般的に，出力層での活性化関数は，回帰タスクでは恒等関数，分類タスクだとsoftmax関数．   \n",
    "恒等関数は説明する必要がないと思いますので省きますが，softmax関数について，以下に示します．   \n",
    "$$y_k = \\frac{\\exp(a_k)}{\\sum{}_{i=1}^{n}\\exp(a_i)}$$\n",
    "この式は，k番目の要素となる確率を表すことになります．   \n",
    "確認してみると分かりますが，すべてのkの確率を足し合わせると，1になるように設計されています．\n",
    "出力層のニューロンの数を，出力として取り得るクラスの数に相当させることで，クラスへの生起確率を取得するという構造になっています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0.3, 2.9, 4.0])\n",
    "\n",
    "exp_a = np.exp(a)\n",
    "print(exp_a)\n",
    "\n",
    "sum_exp_a = np.sum(exp_a)\n",
    "print(sum_exp_a)\n",
    "\n",
    "y = exp_a / sum_exp_a\n",
    "print(y)\n",
    "\n",
    "print(np.sum(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ソフトマックス関数の実装上での問題点\n",
    "#### オーバーフロー\n",
    "コンピュータには，オーバーフローという現象が生じます．   オーバーフローについて簡単に説明すると，値が大きすぎて，コンピュータが1つの数値を表現する為に用意している領域からはみ出ている状態のことです．   \n",
    "#### 指数関数はオーバーフローに注意する必要がある\n",
    "<div>\n",
    "<p>指数関数の増加の度合いは，ほかの一般的な関数に比べて非常に高いです．</p>\n",
    "<img style=\"width: 600px;\" src=\"https://camo.qiitausercontent.com/cbeacd4a39529a3fdbb5e0b422e7a1bb459468e0/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3130363935362f64366164316330622d663866302d373135372d326535342d3130356365626635306237332e6a706567\" />\n",
    "</div>\n",
    "\n",
    "$$y_k = \\frac{\\exp(a_k)}{\\sum_{i=1}^n{\\exp(a_i)}}$$\n",
    "\n",
    "従って，このまま実装すると，ソフトマックス関数の計算途中でオーバーフローを起こしやすそうが分かります．<br>\n",
    "そこで，以下の式から改善案が導かれます\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_k = \\frac{\\exp(a_k)}{\\sum_{i=1}^n{\\exp(a_i)}}$<br><br>\n",
    "$ = \\frac{C\\exp(a_k)}{C\\sum_{i=1}^n{\\exp(a_i)}}$<br><br>\n",
    "$ = \\frac{\\exp(a_k+\\log{C})}{\\sum_{i=1}^n{\\exp(a_i+\\log{C})}}$<br><br>\n",
    "$ = \\frac{\\exp(a_k+C')}{\\sum_{i=1}^n{\\exp(a_i+C')}}$<br><br>\n",
    "ここで導いたように，ソフトマックスの指数関数の計算を行う際には，何かしらの定数を足しても結果が変化しないということが分かります．   \n",
    "つまり，大きくなり過ぎないように，オーバーフローしないように，指数関数部分の入力に定数を足せ（引け）ばいいのです．   \n",
    "オーバーフロー対策としては，入力信号の中の最大値で引くことが一般的です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力がnan(not a number)になっている\n",
    "a = np.array([1010, 1000, 990])\n",
    "np.exp(a) / np.sum(np.exp(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.max(a)\n",
    "a - c\n",
    "np.exp(a - c) / np.sum(np.exp(a - c))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上の議論を踏まえて，ソフトマックス関数を実装してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    # バッチ計算に向けて，色々引数を設定しておく\n",
    "    # keepdimで2次元構造を崩さないようにして，axis=-1で一番小さい次元内で合計をするように指定\n",
    "    c = np.max(a, axis=-1, keepdims=True)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a, axis=-1, keepdims=True)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 3, 5])\n",
    "y = softmax(x)\n",
    "print(y)\n",
    "print(np.sum(y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装上の問題は解決されましたが，実際問題，指数関数の計算はそれなりにコンピュータの計算が必要なので，実際は，推論時のニューラルネットワークでは省略されるケースが多いようです    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出力のニューロン数\n",
    "出力層のニューロン数は，タスクに応じて適宜決定していく必要があります．例えば，手書きの数字を認識したいとなれば，0~9までの数字を認識する必要があるので，出力層のニューロンは10個用意し，他のニューロンと比較した際のそれぞれの値（ソフトマックス関数を適用したのであれば，確率）がそれぞれの数字に帰属する可能性の高さを示しています．   \n",
    "最初の方で掲載した以下の例だと，回帰タスクやある特定のクラスに属する確率を求めたいようなケースで利用できそうですね\n",
    "<div  style='width: 400px; margin-right: 10px;'>\n",
    "    <div>\n",
    "        ↓ニューラルネットワークの例（再掲）\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src='https://image.itmedia.co.jp/ait/articles/2003/24/di-gsdl0102.gif' />\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手書き数字認識\n",
    "### MNISTデータセット\n",
    "MNISTデータセットは，広く利用されているオープンデータで，手書きした数字を沢山持っているデータセットになります   \n",
    "学習用の画像が60,000枚あり，テスト用のデータが10,000枚入っています．    \n",
    "\n",
    "<!-- なお，学習用のデータとテスト用のデータを分ける理由については，数理モデリングの方で -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(f'x_train.shape: {x_train.shape}')\n",
    "print(f'y_train.shape: {y_train.shape}')\n",
    "print(f'x_test.shape: {x_test.shape}')\n",
    "print(f'x_test.shape: {x_test.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回ロードしたデータの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[:5])\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 5, figsize=(8, 8))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(x_train[:25], axs):\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "おまけに言っておくと，画像データも結局は数値データになります．白黒画像だと，それぞれの画素に[0,255]の範囲で値が入り，それに応じて白くしたり黒くしたりしています．   \n",
    "また，カラー画像は色々な表現方法がありますが，Red, Green, Blueの3色を混ぜることで様々な色を表現しており，それぞれの画素に[0, 255]の範囲で(R,G,B)の順番で格納されています"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(x_train[1], cmap='gray', cbar=False, annot=True, fmt='.3g')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つまり，グレースケールの画像はただの２次元配列になるのです．(カラーになると3次元配列になりますが少し難しいと思われますので今回は割愛）  \n",
    "事前に用意されているネットワークを用いて，推論する処理を実装します"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの読み込み関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train.astype('float32') / 255, x_test.astype('float32') / 255\n",
    "    return (x_train, y_train), (x_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 事前に用意されたモデルの重みを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network():\n",
    "    network: dict\n",
    "    weights: np.ndarray\n",
    "\n",
    "    # 事前に用意しておいたモデルを持ってきます\n",
    "    def get_weights():\n",
    "        weights = None\n",
    "        if os.path.exists(\"./about_model/weights.npy\"):\n",
    "            weights = np.load(\"./about_model/weights.npy\", allow_pickle=True)\n",
    "        elif os.path.exists(\"./dl-lec/03/about_model/weights.npy\"):\n",
    "            weights = np.load(\"./dl-lec/03/about_model/weights.npy\", allow_pickle=True)\n",
    "        else:\n",
    "            !git clone https://github.com/dakken205/dl-lec.git\n",
    "            weights = np.load(\"./dl-lec/03/about_model/weights.npy\", allow_pickle=True)\n",
    "        return weights\n",
    "\n",
    "    weights = get_weights()\n",
    "    network = {}\n",
    "    network['W1'] = weights[0]\n",
    "    network['b1'] = weights[1]\n",
    "    network['W2'] = weights[2]\n",
    "    network['b2'] = weights[3]\n",
    "    network['W3'] = weights[4]\n",
    "    network['b3'] = weights[5]\n",
    "\n",
    "    return network\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論（予測）フェーズの為の関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, x):\n",
    "    # 与えられたネットワーク重み取得\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    # 24x24の2次元配列になっており，都合が悪いので，ベクトルに変換\n",
    "    x = x.reshape(-1, 28*28)\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "    return y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用意した関数を用いて計算してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = get_data()\n",
    "network = init_network()\n",
    "for i in range(10):\n",
    "    # データを1つずつ取得\n",
    "    x = x_test[i]\n",
    "    y = predict(network, x)\n",
    "    print('{}個目のデータ -> 予測ラベル：{}, 正解ラベル：{}'.format(i, y.argmax(), y_test[i]))\n",
    "\n",
    "y.sum()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バッチ計算\n",
    "上のコードで，機能的な要件は満たしているが，実はループで計算するよりも，多次元配列で並列的に計算した方がより効率的です（他にもまとめて計算した方が良い理由はありますが）．  \n",
    "そこで，データを一つ一つ与えるのではなく，複数のデータを1つの要素とした配列を構成し，一回のループで一気に複数データを計算するような仕組みをバッチ計算と言います"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = network['W1']\n",
    "W2 = network['W2']\n",
    "W3 = network['W3']\n",
    "print(W1.shape)\n",
    "print(W2.shape)\n",
    "print(W3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([1, 2, 5, 4])\n",
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, _), (x_test, y_test) = get_data()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "accuracy_cnt = 0\n",
    "\n",
    "for i in range(0, len(x_test), batch_size):\n",
    "    # ここでバッチに分けています\n",
    "    x_batch = x_test[i:i+batch_size]\n",
    "\n",
    "    y_batch = predict(network, x_batch)\n",
    "    # バッチごとに，確率最大となっているラベル（予測ラベル）を取得\n",
    "    p = np.argmax(y_batch, axis=1)\n",
    "    # 正解ラベルと一致している個数の取得\n",
    "    accuracy_cnt += np.sum(p == y_test[i:i+batch_size])\n",
    "\n",
    "print('Accuracy: ' + str(float(accuracy_cnt) / len(x_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
